{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0f84710-fa07-4ae7-a37f-f933992a5183",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fea5d599-883a-41b7-9084-a2c54697cd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "import numpy.linalg as la\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dask.distributed import Client, LocalCluster, as_completed\n",
    "from dask import delayed\n",
    "from runpy import run_path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "import networkx as nx\n",
    "import raphtory as rp\n",
    "import umap.umap_ as umap\n",
    "import streamlit as st\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import local2global as l2g # ADDED\n",
    "\n",
    "import torch\n",
    "import torch_geometric as tg\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils.convert import from_networkx\n",
    "from torch_geometric.transforms import LargestConnectedComponents\n",
    "from torch_geometric.utils import to_networkx, from_networkx, one_hot\n",
    "#from torch_geometric.nn import Node2Vec, GCNConv, VGAE\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4958605c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import l2gl\n",
    "from l2gl.embedding import GAE, GAE_loss\n",
    "from l2gl.utils import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4f7e21f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"../data/nas/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148e6778-b3bc-437a-b023-db0d538c05ff",
   "metadata": {},
   "source": [
    "# <font color=\"grey\"> Autonomous Systems</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc356df-39f0-4f1d-b98d-d4dcfe4665db",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to walk through the graph embedding and alignment process in a self-contained way. The full existing Local2Global package is available [here](https://github.com/LJeub/Local2Global_embedding) and the expectation is to pick parts from it as a starting point. It is also available in on this repository in the Local2Global_embedding folder for reference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4208fd-da21-4181-ae65-1fa8c152ea2c",
   "metadata": {},
   "source": [
    "### <font color=\"grey\">  Table of Contents</font>\n",
    "\n",
    "1. #### <a href='#chapter1'>Data</a>\n",
    "2. #### <a href='#chapter2'>Embedding</a>\n",
    "3. #### <a href='#chapter3'>Visualisation</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d903eb7-22ad-4900-8720-f79d3eb44790",
   "metadata": {},
   "source": [
    "###  <a id='chapter1'> <font color=\"grey\">1. Data </font></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d25865e-21a9-4e34-bf80-125bb3d22ca5",
   "metadata": {},
   "source": [
    "The data can be accessed via the dataloader. It is saved in the datasets/data/nas directory in two parquet files. There are many alternative ways of doing this. One option to explore is to have the datasets available as in [torch_geometric datasets](https://pytorch-geometric.readthedocs.io/en/2.6.0/modules/datasets.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38fcc940-390a-4039-aa4b-c136185c16d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/u1774790/Projects/Graphs/code/l2glite/data\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(dset='nas')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcfd3ee-0e5d-4a5e-aa09-30e0dc958132",
   "metadata": {},
   "source": [
    "The data is stored in one dataframe for the nodes (including all the features) and one for the edges (including edge weights)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9273655-0c7c-4f2f-b40a-574d38773307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 7)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>timestamp</th><th>nodes</th><th>nodetype</th><th>country</th><th>asname</th><th>nodename</th><th>cc</th></tr><tr><td>datetime[μs]</td><td>i64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>i64</td></tr></thead><tbody><tr><td>2024-09-14 00:00:00</td><td>0</td><td>&quot;asn&quot;</td><td>&quot;US&quot;</td><td>&quot;WINDSTREAM&quot;</td><td>&quot;AS7029&quot;</td><td>235</td></tr><tr><td>2024-09-14 00:00:00</td><td>1</td><td>&quot;asn&quot;</td><td>&quot;US&quot;</td><td>&quot;RUELALA-INC&quot;</td><td>&quot;AS32984&quot;</td><td>235</td></tr><tr><td>2024-09-14 00:00:00</td><td>2</td><td>&quot;asn&quot;</td><td>&quot;ID&quot;</td><td>&quot;FIBERSTAR-AS-I&quot;</td><td>&quot;AS136106&quot;</td><td>104</td></tr><tr><td>2024-09-14 00:00:00</td><td>3</td><td>&quot;asn&quot;</td><td>&quot;ID&quot;</td><td>&quot;HSPNET-AS-I&quot;</td><td>&quot;AS58495&quot;</td><td>104</td></tr><tr><td>2024-09-14 00:00:00</td><td>4</td><td>&quot;asn&quot;</td><td>&quot;US&quot;</td><td>&quot;BTN-ASN&quot;</td><td>&quot;AS3491&quot;</td><td>235</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 7)\n",
       "┌─────────────────────┬───────┬──────────┬─────────┬────────────────┬──────────┬─────┐\n",
       "│ timestamp           ┆ nodes ┆ nodetype ┆ country ┆ asname         ┆ nodename ┆ cc  │\n",
       "│ ---                 ┆ ---   ┆ ---      ┆ ---     ┆ ---            ┆ ---      ┆ --- │\n",
       "│ datetime[μs]        ┆ i64   ┆ str      ┆ str     ┆ str            ┆ str      ┆ i64 │\n",
       "╞═════════════════════╪═══════╪══════════╪═════════╪════════════════╪══════════╪═════╡\n",
       "│ 2024-09-14 00:00:00 ┆ 0     ┆ asn      ┆ US      ┆ WINDSTREAM     ┆ AS7029   ┆ 235 │\n",
       "│ 2024-09-14 00:00:00 ┆ 1     ┆ asn      ┆ US      ┆ RUELALA-INC    ┆ AS32984  ┆ 235 │\n",
       "│ 2024-09-14 00:00:00 ┆ 2     ┆ asn      ┆ ID      ┆ FIBERSTAR-AS-I ┆ AS136106 ┆ 104 │\n",
       "│ 2024-09-14 00:00:00 ┆ 3     ┆ asn      ┆ ID      ┆ HSPNET-AS-I    ┆ AS58495  ┆ 104 │\n",
       "│ 2024-09-14 00:00:00 ┆ 4     ┆ asn      ┆ US      ┆ BTN-ASN        ┆ AS3491   ┆ 235 │\n",
       "└─────────────────────┴───────┴──────────┴─────────┴────────────────┴──────────┴─────┘"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the nodes\n",
    "node_df = dl.get_nodes()\n",
    "node_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "149a77d9-d83c-45ff-a933-a774c50da54b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>timestamp</th><th>source</th><th>dest</th><th>weight</th></tr><tr><td>datetime[μs]</td><td>i64</td><td>i64</td><td>i64</td></tr></thead><tbody><tr><td>2024-09-14 00:00:00</td><td>0</td><td>1</td><td>1</td></tr><tr><td>2024-09-14 00:00:00</td><td>0</td><td>465</td><td>1</td></tr><tr><td>2024-09-14 00:00:00</td><td>0</td><td>596</td><td>1</td></tr><tr><td>2024-09-14 00:00:00</td><td>0</td><td>1234</td><td>1</td></tr><tr><td>2024-09-14 00:00:00</td><td>0</td><td>1272</td><td>1</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 4)\n",
       "┌─────────────────────┬────────┬──────┬────────┐\n",
       "│ timestamp           ┆ source ┆ dest ┆ weight │\n",
       "│ ---                 ┆ ---    ┆ ---  ┆ ---    │\n",
       "│ datetime[μs]        ┆ i64    ┆ i64  ┆ i64    │\n",
       "╞═════════════════════╪════════╪══════╪════════╡\n",
       "│ 2024-09-14 00:00:00 ┆ 0      ┆ 1    ┆ 1      │\n",
       "│ 2024-09-14 00:00:00 ┆ 0      ┆ 465  ┆ 1      │\n",
       "│ 2024-09-14 00:00:00 ┆ 0      ┆ 596  ┆ 1      │\n",
       "│ 2024-09-14 00:00:00 ┆ 0      ┆ 1234 ┆ 1      │\n",
       "│ 2024-09-14 00:00:00 ┆ 0      ┆ 1272 ┆ 1      │\n",
       "└─────────────────────┴────────┴──────┴────────┘"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_df = dl.get_edges()\n",
    "edge_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47ee1fa-6bd2-41ae-bd74-36d3af72bdbe",
   "metadata": {},
   "source": [
    "Ultimately, working with the people at Pometry, we want to use the [Raphtory](https://www.raphtory.com/) graph format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c87a8252-ff89-4fe2-8a37-fd4e9cb0154f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4f907d4994a4fdfbc8b3b3cfc675541",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), IntProgress(value=0, max=16717484), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8784242618740d0b7a0f017d5539129",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), IntProgress(value=0, max=2538974), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Raphtory format\n",
    "g = dl.get_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca43c1c-c500-4597-b07e-61e751c6aa66",
   "metadata": {},
   "source": [
    "The Raphtory formal is still work in progress but one can contribute to their code (based in Rust), contribute to the discussion on their Slack (linked on their page) or directly get in touch with [Lucas Jeub](https://github.com/LJeub) and/or Ben Steer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d399ac6b-0eb2-40ea-bb32-37d6b0f0892d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats on the graph structure:\n",
      "Number of nodes (AS nodes): 85428\n",
      "Number of unique edges (src,dst): 914346\n",
      "Total interactions (edge updates): 16717484\n",
      "Stats on the graphs time range:\n",
      "Earliest datetime: 2024-09-14 00:00:00+00:00\n",
      "Latest datetime: 2024-10-13 00:00:00+00:00\n"
     ]
    }
   ],
   "source": [
    "print(\"Stats on the graph structure:\")\n",
    "\n",
    "number_of_nodes = g.count_nodes()\n",
    "number_of_edges = g.count_edges()\n",
    "total_interactions = g.count_temporal_edges()\n",
    "\n",
    "print(\"Number of nodes (AS nodes):\", number_of_nodes)\n",
    "print(\"Number of unique edges (src,dst):\", number_of_edges)\n",
    "print(\"Total interactions (edge updates):\", total_interactions)\n",
    "\n",
    "print(\"Stats on the graphs time range:\")\n",
    "\n",
    "earliest_datetime = g.earliest_date_time\n",
    "latest_datetime = g.latest_date_time\n",
    "\n",
    "print(\"Earliest datetime:\", earliest_datetime)\n",
    "print(\"Latest datetime:\", latest_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15a91cb9-ff6a-4fcd-8fbc-aecde67fee67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The node features are:  ['asname', 'cc', 'country', 'nodename', 'nodetype']\n"
     ]
    }
   ],
   "source": [
    "print(\"The node features are: \", g.nodes.properties.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf8afc9-aaa4-4bfc-a99a-4dcda9795f0b",
   "metadata": {},
   "source": [
    "The graphs we are dealing are **temporal**, meaning that nodes and edges have timestamps. One can interpret this as having one graph for each point in time, with a possible overlap of nodes and edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "994cb72f-23c6-4151-b0fd-bf4b30340a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = dl.get_dates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839cff93-80cf-42f4-bfe4-ed4234e88a4d",
   "metadata": {},
   "source": [
    "For this particular dataset, the graph for each day represents a patch. In order to apply graph neural networks to each patch, we need to process these into the Data format used by pytorch-geometric, described [here](https://pytorch-geometric.readthedocs.io/en/latest/get_started/introduction.html). In particular, for each patch we need to enumerate the nodes and use these indices to designate the nodes. We need a dictionary that maps the nodes in each patch to their names and we need to encode the node and edge features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ad726d0-98c2-4e50-b173-b4a81b4e547e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode nodes present at each date\n",
    "nodes = {}\n",
    "node_dict = {}\n",
    "for d in dates:\n",
    "    nodes[d] = dl.get_node_list(ts=d)\n",
    "    node_dict[d] = dict(zip(nodes[d],range(len(nodes[d]))))\n",
    "all_nodes = dl.get_node_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e51fabc9-a781-4512-adf4-dcb15276a250",
   "metadata": {},
   "outputs": [],
   "source": [
    "numbering_nodes= {x : i for i, x in enumerate(all_nodes)}\n",
    "list_nodes=list(nodes.values())\n",
    "\n",
    "list_nodes_renumbered=[]\n",
    "for l in list_nodes:\n",
    "    list_nodes_renumbered.append( [numbering_nodes[i] for i in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b82e628b-08e8-4aa0-aa62-6b25d2c1e9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode country codes\n",
    "cc = pl.read_csv(PATH+'country_codes.csv')\n",
    "countrycode_dict = dict(zip(cc[\"alpha-2\"].to_list(), range(cc.shape[0])))\n",
    "#cc_one_hot = one_hot(torch.tensor(list(countrycode_dict.values()), dtype=torch.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f9d614f4-7a6f-4b24-b743-e4591a1ec16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign country code index to each node. The way this is done is a bit convoluted, as some nodes are assigned to both a country and to 'ZZ'\n",
    "# in the database, so we need to fix that. This should be done in pre-processing\n",
    "df = dl.get_nodes().with_columns(\n",
    "    pl.col(\"country\").replace(old=pl.Series(countrycode_dict.keys()), new=pl.Series(countrycode_dict.values())).cast(pl.Int64).alias('cc')\n",
    ").select([\"nodes\", \"cc\"]).group_by(\"nodes\").agg(pl.col(\"cc\").min().cast(pl.Int64).alias(\"cc\")).sort([\"cc\",\"nodes\"])\n",
    "node_cc_dict = dict(zip(df[\"nodes\"].to_list(), df[\"cc\"].to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4f87d7aa-70de-4bdc-b54e-4ba41b655c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For every day, create a list of node features\n",
    "features = {}\n",
    "for d in dates:\n",
    "    features[d] = one_hot(torch.tensor(dl.get_nodes(ts=d).select(\n",
    "        pl.col(\"country\").replace(old=pl.Series(countrycode_dict.keys()), new=pl.Series(countrycode_dict.values())).cast(pl.Int64)\n",
    "    ).to_numpy().flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1470fe0a-a8b6-4151-8e1e-2fcd210b13d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([84575, 250])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[dates[3]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3dccedc3-6521-4158-90bf-aba33f2d902e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba96e126de3e4a42826b288f88b23621",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create pytorch-geometric Data object\n",
    "tg_graphs = {}\n",
    "for d in tqdm(dates):\n",
    "    edges = dl.edges.filter(pl.col('timestamp')==d).select(\n",
    "        pl.col('source').replace(old=pl.Series(node_dict[d].keys()), new=pl.Series(node_dict[d].values())).cast(pl.Int64),\n",
    "        pl.col('dest').replace(old=pl.Series(node_dict[d].keys()), new=pl.Series(node_dict[d].values())).cast(pl.Int64)\n",
    "    ).to_numpy()\n",
    "    edge_index = torch.tensor([tuple(x) for x in edges], dtype=torch.long).t().contiguous()\n",
    "    tgraph = Data(edge_index=edge_index)\n",
    "    # Add features - problem is that for the embedding we only want those present at a given time\n",
    "    tgraph.x = features[d]\n",
    "    tg_graphs[d] = tgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "692f927b-87d3-4ca3-885b-fab11ee7e71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, d in enumerate(dates):\n",
    "    tg_graphs[d].nodes=torch.tensor(list_nodes_renumbered[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1f3c578c-6df9-4437-9121-ef9d1e675235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select one date to test embedding\n",
    "data = tg_graphs[dates[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762d7280-9de2-4f2d-8e5d-daf738cf928f",
   "metadata": {},
   "source": [
    "###  <a id='chapter2'> <font color=\"grey\">2. Embedding </font></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faab9f1c-2fd1-4f6f-bd65-8d8ce24185ab",
   "metadata": {},
   "source": [
    "For the embedding, we use the architecture of a Variational Graph Autoencoder. Given a graph $G=(V,E)$ with $|V|=n$ nodes and node features ${x}_i\\in \\mathbb{R}^d$, $i\\in [n]$, denote by ${X}=[{x}_1,\\dots,{x}_n]^T\\in \\mathbb{R}^{n\\times d}$ the features matrix and by $A=(a_{ij})\\in \\{0,1\\}^{n\\times n}$ the adjacency matrix of the graph. The **encoder** produces latent representations ${z}_i\\in \\mathbb{R}^k$ for $i\\in [n]$, which are sampled from the inference model\n",
    "\\begin{equation*}\n",
    "  q({z}_i \\ | \\ {X},{A}) = \\mathcal{N}({z}_i \\ | \\ {\\mu}_i,\\mathrm{diag}({\\sigma}_i)).\n",
    "\\end{equation*}\n",
    "The means $\\mu_i$ and variances $\\mathrm{diag}({\\sigma}_i)$ are parametrized using an encoder network, for example, a graph convolutional neural network (GCN). Denoting by ${Z}=[{z}_1,\\dots,{z}_n]^T$ the matrix of latent represenations and by ${\\mu}$ and ${\\sigma}$ the matrices representing the means and variances, we have\n",
    "\\begin{equation*}\n",
    "  {\\mu} = \\mathrm{GCN}_{\\mu}({X},{A}), \\quad \\quad \\log {\\sigma} = \\mathrm{GCN}_{\\sigma}({X},{A}).\n",
    "\\end{equation*}\n",
    "The **generative model** is a distribution on the adjacency matrix,\n",
    "\\begin{equation*}\n",
    "  p({A}\\ | \\ {Z}) = \\prod_{i,j} p(a_{ij} \\ | \\ {z}_i,{z}_j).\n",
    "\\end{equation*}\n",
    "It is convenient to use\n",
    "\\begin{equation*}\n",
    "  p(a_{ij}=1 \\ | \\ {z}_i,{z}_j) = \\sigma({z}_i^T{z}_j),\n",
    "\\end{equation*}\n",
    "where $\\sigma$ is the logistic sigmoid. In order to train the model, we optimize the evidence lower bound\n",
    "\\begin{equation*}\n",
    "  \\mathcal{L} = \\mathbb{E}_{q({Z}\\ | \\ {X},{A})}[\\log p({A}\\ | \\ {Z})]-\\mathrm{D}_{\\mathrm{KL}}(q({Z}\\ | \\ {X},{A}) \\ \\| \\ p({Z})).\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "03bdf91f-2d14-45e4-999b-1202d5ac45d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GAE(dim=64, hidden_dim=128, num_features=data.num_node_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5952dc3e-2d6d-4070-8e68-09e827820147",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data, model, loss_fun, num_epochs=100, verbose=True, lr=0.01, logger=lambda loss: None):\n",
    "    losses = []\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "    # schedule = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_epochs)\n",
    "    for e in tqdm(range(num_epochs)):\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fun(model, data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(float(loss))\n",
    "        if verbose:\n",
    "            if not e % 20:\n",
    "                print(f'epoch {e}: loss={loss.item()}')\n",
    "        # schedule.step()\n",
    "    return model, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "613527ab-a063-4283-bd62-5ee3b58121a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ad3980161ec4cf5b2169f4cbc974e58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: loss=1.3465496301651\n",
      "epoch 20: loss=0.9173072576522827\n",
      "epoch 40: loss=0.8851785659790039\n",
      "epoch 60: loss=0.8646342754364014\n",
      "epoch 80: loss=0.8525534868240356\n"
     ]
    }
   ],
   "source": [
    "model, losses = train(data, model, GAE_loss, num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2ab6bc1b-7f0b-440e-bb1e-193136cecac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eaa00692-b04c-45fc-b44f-f8db54d5b4be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84592, 64)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = model.encode(data).detach().numpy()\n",
    "embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822c64b9-04a9-48c8-90b1-41bff00a7de5",
   "metadata": {},
   "source": [
    "In the original L2G code, there is a Patch class handling patches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad01511-5965-4676-8b56-dd304d2ec538",
   "metadata": {},
   "source": [
    "###  <a id='chapter3'> <font color=\"grey\">3. Visualisation </font></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9cd75dab-3105-454f-a615-df326670871b",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common = ['AU', 'BR', 'CN', 'DE', 'IN', 'ID', 'PL', 'RU', 'GB', 'US']\n",
    "countries = dl.get_nodes(ts=dates[0])['country'].to_list()\n",
    "indices = [i for i in range(len(countries)) if countries[i] in most_common]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4961ad0e-e22e-4160-a742-4e461bfa4741",
   "metadata": {},
   "outputs": [],
   "source": [
    "points = embedding[indices, :]\n",
    "labels = [most_common.index(countries[i]) for i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "24c57c17-115a-4aee-8d9f-cd69d89901e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use UMAP to visualise the graph embeddings for different days\n",
    "reducer = umap.UMAP(n_neighbors=5, min_dist=0.0, metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d8ede66a-8b07-4a10-8121-9645d8f5dd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#points = StandardScaler().fit_transform(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ab359a-4cd7-4d30-8c1f-f38a7fac2b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b68f64-9fd7-4ad5-8aad-d0939325a330",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_embedding = reducer.fit_transform(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ba61ce-a601-44ad-af31-488757c060b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12,12))\n",
    "ax.scatter(\n",
    "    points[:, 0],\n",
    "    points[:, 1],\n",
    "    c=[sns.color_palette()[x] for x in labels],\n",
    "    lw=1\n",
    ")\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "plt.title('UMAP Embedding', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159fcea7-f8e9-49d6-b371-ec3128dac17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12,12))\n",
    "ax.scatter(\n",
    "    umap_embedding[:, 0],\n",
    "    umap_embedding[:, 1],\n",
    "    c=[sns.color_palette()[x] for x in labels],\n",
    "    lw=1\n",
    ")\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "plt.title('UMAP Embedding', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d47bb4-4345-497e-9fdf-a0a2a0ac41c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12,12))\n",
    "ax.scatter(\n",
    "    umap_embedding[:, 0],\n",
    "    umap_embedding[:, 1],\n",
    "    c=[sns.color_palette()[x] for x in labels],\n",
    "    lw=1\n",
    ")\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "plt.title('UMAP Embedding', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ef4871-a108-4071-8411-0ccbcfac5731",
   "metadata": {},
   "outputs": [],
   "source": [
    "#patch_list = []\n",
    "models = []\n",
    "embeddings = []\n",
    "patch_emb=[] #ADDED\n",
    "for d in dates[:5]:\n",
    "    patch = tg_graphs[d]\n",
    "    model = VGAE(encoder=Encoder(64, patch.num_node_features))\n",
    "    model, _ = train(patch, model, VGAE_loss, num_epochs=5, lr=0.01)\n",
    "    patch_emb.append(l2g.Patch(patch.nodes, model.encode(patch).detach().numpy())) #ADDED\n",
    "    #coordinates = model.encode(patch).detach().numpy()\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895dffe3-dad3-4307-a4c0-416cc8d65aca",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fde5e3-f417-49d8-b884-62d2a8d6f83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADDED \n",
    "## HOW TO GET THE TOTAL EMBEDDING WITH NEW VERSION OF THE ALGORITHM\n",
    "\n",
    "import manopt_optimization as mopt\n",
    "intersection_nodes=mopt.double_intersections_nodes(patch_emb)\n",
    "\n",
    "dim=64\n",
    "res, emb =mopt.optimization(patch_emb, intersection_nodes, dim) #res contain the result of the optimization, i.e., scales, rotations and traslations,\n",
    "                                                                # emb is the embedding of every nodes using the scales, rotations and translations \n",
    "                                                                #found with  the optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0585d4-2174-4834-81ce-471a65bec069",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fc6c14-f13f-4572-b854-7080aa0fc8ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd4bde3-651c-4dd5-9a8f-1dcc92e20ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ADDED \n",
    "#TO GET THE EMBEDDING USING STANDARD L2G\n",
    "pr=l2g.AlignmentProblem(patch_emb)\n",
    "old_emb=pr.get_aligned_embedding()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff0a1c6-1b07-4ea0-88b5-4474fdb3699c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d670d22-c6cb-42b0-9f02-b188f29e5302",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_data(data, date, most_common):\n",
    "    countries = dl.get_nodes(ts=date)['country'].to_list()\n",
    "    indices = [i for i in range(len(countries)) if countries[i] in most_common]\n",
    "    points = data[indices, :]\n",
    "    labels = [most_common.index(countries[i]) for i in indices]\n",
    "    return points, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee98c96-b390-43d3-88ae-1a1b5f0bc10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_plot(umap_embedding, labels, p):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12,12))\n",
    "    ax.scatter(\n",
    "        umap_embedding[p][:, 0],\n",
    "        umap_embedding[p][:, 1],\n",
    "        c=[sns.color_palette()[x] for x in labels[p]],\n",
    "        lw=1\n",
    "    )\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1999cb59-72ad-4195-90fb-81de8b8bcc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP(n_neighbors=5, min_dist=0.0, metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600815b4-92b2-4bb3-8e59-73e4201478a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "for i,d in enumerate(dates):\n",
    "    patch = tg_graphs[d]\n",
    "    embeddings.append(models[i].encode(patch).detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c861a582-44cf-407c-9984-6c2b6d5b9b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_umap(points, reducer):\n",
    "    return reducer.fit(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37a1a99-0128-4da5-8d25-81705248df4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP(n_neighbors=5, min_dist=0.0, metric='euclidean')\n",
    "reducer.fit_transform(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fc1254-925a-42eb-a9ae-cf8cf17b80fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "points = []\n",
    "labels = []\n",
    "umaps = []\n",
    "for i,d in enumerate(dates):\n",
    "    p, l = reduce_data(embeddings[i], d, most_common)\n",
    "    points.append(p)\n",
    "    labels.append(l)\n",
    "    umaps.append(create_umap(p, reducer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1d8ce1-4bce-4089-ad7a-0a6974716bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12,12))\n",
    "ax.scatter(\n",
    "    umaps[1][:, 0],\n",
    "    umaps[1][:, 1],\n",
    "    c=[sns.color_palette()[x] for x in labels[1]],\n",
    "    lw=1\n",
    ")\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "plt.title('UMAP Embedding', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c83195-a565-44f0-a1d1-f1ca34a1f4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4, 1, figsize=(8,20))\n",
    "for i in range(4):\n",
    "    ax[i].scatter(\n",
    "        umaps[i][:, 0],\n",
    "        umaps[i][:, 1],\n",
    "        c=[sns.color_palette()[x] for x in labels[i]],\n",
    "        lw=1\n",
    "    )\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "plt.title('UMAP Embedding', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802c6faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim=128\n",
    "#patch_list = []\n",
    "models = []\n",
    "embeddings = []\n",
    "patch_emb=[] #ADDED\n",
    "for d in dates:\n",
    "    patch = tg_graphs[d]\n",
    "    model = VGAE(encoder=Encoder(dim, patch.num_node_features))\n",
    "    model, _ = train(patch, model, VGAE_loss, num_epochs=40, lr=0.01)\n",
    "    patch_emb.append(Patch(patch.nodes, model.encode(patch).detach().numpy())) #ADDED\n",
    "    #coordinates = model.encode(patch).detach().numpy()\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8046a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j, p in enumerate(patch_emb):\n",
    "    p_countries = dl.get_nodes(ts=dates[j])['country'].to_list()\n",
    "    p_indices = [i for i in range(len(p_countries)) if p_countries[i] in most_common]\n",
    "    p_labels = [most_common.index(p_countries[i]) for i in p_indices]\n",
    "    p_points=p.coordinates[p_indices, :]\n",
    "    umap_embedding = reducer.fit_transform(p_points)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12,12))\n",
    "    ax.scatter(\n",
    "        umap_embedding[:, 0],\n",
    "        umap_embedding[:, 1],\n",
    "        c=[sns.color_palette()[x] for x in p_labels],\n",
    "        lw=1\n",
    "    )\n",
    "    plt.gca().set_aspect('equal', 'datalim')\n",
    "    plt.title('UMAP Embedding', fontsize=12)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad870c54",
   "metadata": {},
   "source": [
    "## NEW VERSION OF ALIGNMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b3fb79cc-708a-4766-abe4-c85ce5a33103",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m     emb_list\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(itertools\u001b[38;5;241m.\u001b[39mchain\u001b[38;5;241m.\u001b[39mfrom_iterable(emb_list))\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m emb_list    \n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mAffineTransform\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, patch_emb, dim, n_patches):\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;28msuper\u001b[39m(AffineTransform, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "def double_intersections_nodes(patches):\n",
    "    double_intersections=dict()\n",
    "    for i in range(len(patches)):\n",
    "        for j in range(i+1, len(patches)):\n",
    "            double_intersections[(i,j)]=list(set(patches[i].nodes.tolist()).intersection(set(patches[j].nodes.tolist())))\n",
    "    return double_intersections\n",
    "\n",
    "def preprocess_graphs(list_of_patches, nodes_dict):\n",
    "    emb_list=[]\n",
    "    for i in range(len(list_of_patches)-1):\n",
    "        emb_list.append([torch.tensor(list_of_patches[i].get_coordinates(list(nodes_dict[i,i+1]))),\n",
    "                         torch.tensor(list_of_patches[i+1].get_coordinates(list(nodes_dict[i,i+1])))])\n",
    "    emb_list=list(itertools.chain.from_iterable(emb_list))\n",
    "    return emb_list    \n",
    "\n",
    "class AffineTransform(nn.Module):\n",
    "    def __init__(self, patch_emb, dim, n_patches):\n",
    "        super(AffineTransform, self).__init__()\n",
    "        \n",
    "        self.R = nn.ParameterList([nn.Parameter(torch.randn(dim, dim)) for _ in range(n_patches)])\n",
    "        self.s=nn.ParameterList([1] + [nn.Parameter(torch.randn(1)) for _ in range(n_patches-1)])\n",
    "        self.t = nn.ParameterList([nn.Parameter(torch.randn(dim)) for _ in range(n_patches)])\n",
    "        \n",
    "        # Parameters for Y transformation: W2 (d x d), b2 (d)\n",
    "        #self.R2 = nn.Parameter(torch.randn(d, d))\n",
    "        #self.s2=nn.Parameter(torch.randn(1))\n",
    "        #self.t2 = nn.Parameter(torch.randn(d))\n",
    "\n",
    "    def forward(self, patch_emb, n_patches):\n",
    "        \n",
    "        m=2*n_patches -2\n",
    "        R = [self.R[0]] + [item for i in range(1, n_patches) for item in (self.R[i], self.R[i])]\n",
    "        s=[self.s[0]] + [item for i in range(1, n_patches) for item in (self.s[i], self.s[i])]\n",
    "        t= [self.t[0]] + [item for i in range(1, n_patches) for item in (self.t[i], self.t[i])]\n",
    "\n",
    "        \n",
    "            \n",
    "        transformed_emb= [patch_emb[i] @ R[i] + t[i] for i in range(m)]\n",
    "        \n",
    "        \n",
    "        return transformed_emb \n",
    "\n",
    "\n",
    "# Loss function\n",
    "def loss_function(transformed_emb, rots):\n",
    "    d = rots[0].shape[0]\n",
    "    I = torch.eye(d)\n",
    "    m=len(transformed_emb)\n",
    "    \n",
    "    diff=[transformed_emb[i] - transformed_emb[i+1] for i in range(0, m-1, 2)]\n",
    "    \n",
    "    l=sum([torch.norm(d)**2 for d in diff])\n",
    "\n",
    "    ort_r=sum([torch.norm(R@R.T - I) for R in rots])\n",
    "    loss=l+ort_r\n",
    "   \n",
    "    \n",
    "    return loss\n",
    "\n",
    "def train_model(patch_emb, dim, n_patches , num_epochs=1000, learning_rate=0.01):\n",
    "    loss_hist=[]\n",
    "    \n",
    "    \n",
    "    model = AffineTransform(patch_emb, dim, n_patches)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "     \n",
    "        transformed_patch_emb = model(patch_emb, n_patches)\n",
    "        \n",
    "      \n",
    "        loss = loss_function(transformed_patch_emb, model.R)\n",
    "        \n",
    "     \n",
    "        \n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        loss_hist.append(loss.item())\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "    \n",
    "\n",
    "    return model, loss_hist\n",
    "\n",
    "def get_embedding(patches, trained_model):\n",
    "    scales=[1]+ [ s.detach().numpy().item() for s in trained_model.s[1:]] \n",
    "    rots=[r.detach().numpy() for r in trained_model.R]\n",
    "    shifts=[ s.detach().numpy() for s in trained_model.t] \n",
    "\n",
    "    emb_problem = l2g.AlignmentProblem(patches)\n",
    "    embedding = np.empty((emb_problem.n_nodes, emb_problem.dim))\n",
    "    for node, patch_list in enumerate(emb_problem.patch_index):\n",
    "        embedding[node] = np.mean([scales[i]*emb_problem.patches[p].get_coordinate(node)@rots[i] + shifts[i] for i, p in enumerate(patch_list)], axis=0)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775738d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10000\n",
    "learning_rate = 0.5\n",
    "n_patches=len(patch_emb)\n",
    "nodes=double_intersections_nodes(patch_emb)\n",
    "emb_patches=preprocess_graphs(patch_emb, nodes)\n",
    "\n",
    "model, loss_hist=train_model(emb_patches, dim, n_patches , num_epochs=10000, learning_rate=0.01)\n",
    "\n",
    "embedding=get_embedding(patches, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
